{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66107a1b-64fc-4901-9e55-13e6e5fc5ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df = spark.createDataFrame(range(10),IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea349733-9ebe-46e7-be5a-9cb84e552351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Partitions:  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Default Partitions: \", df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81140f5f-7bb6-40fb-a926-8a47232925f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: [[Row(value=0)],\n [Row(value=1)],\n [Row(value=2)],\n [Row(value=3), Row(value=4)],\n [Row(value=5)],\n [Row(value=6)],\n [Row(value=7)],\n [Row(value=8), Row(value=9)]]"
     ]
    }
   ],
   "source": [
    "# This will show how the data get partitioned in the 8 Partitions\n",
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebd8473d-3def-4215-8482-3d684c317e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The Fundamental concept of Spark DataFrames is basically runs of Partitions, lets say if we have 1000 pages book, we just dont put all the necessary content in just one single page, instead we put it all the 1000 pages equally, in the same way we divide DataFrames into equal partitions and process them parllelly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c1db210-a882-46db-a10e-902257882d82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## In Pyspark, we can do partitions in two different ways\n",
    "## - repartition()\n",
    "## - coalesce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1e39d6b-0fbb-4834-8540-4ba90be63019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Advantage             | Why It Matters                      |\n",
    "| --------------------- | ----------------------------------- |\n",
    "| Parallelism           | Speeds up processing using all CPUs |\n",
    "| Resource efficiency   | Keeps all workers busy and balanced |\n",
    "| Control output files  | Fewer, bigger files = faster I/O    |\n",
    "| Better joins/grouping | Less shuffling, more efficient      |\n",
    "| Smart data storage    | Faster queries on partitioned files |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cbd85f1-be11-42d7-8640-7fa7acf6e782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[31]: [FileInfo(path='dbfs:/FileStore/tables/Online_Retail-1.csv', name='Online_Retail-1.csv', size=46133248, modificationTime=1744228374000),\n FileInfo(path='dbfs:/FileStore/tables/Online_Retail-2.csv', name='Online_Retail-2.csv', size=46133248, modificationTime=1744228637000),\n FileInfo(path='dbfs:/FileStore/tables/Online_Retail-3.csv', name='Online_Retail-3.csv', size=46133248, modificationTime=1744229258000),\n FileInfo(path='dbfs:/FileStore/tables/Online_Retail-4.csv', name='Online_Retail-4.csv', size=46133248, modificationTime=1744229659000),\n FileInfo(path='dbfs:/FileStore/tables/Online_Retail-5.csv', name='Online_Retail-5.csv', size=46133248, modificationTime=1744313114000),\n FileInfo(path='dbfs:/FileStore/tables/Online_Retail.csv', name='Online_Retail.csv', size=46133248, modificationTime=1744221409000),\n FileInfo(path='dbfs:/FileStore/tables/Online_Retail.xlsx', name='Online_Retail.xlsx', size=23715344, modificationTime=1744220158000),\n FileInfo(path='dbfs:/FileStore/tables/retail/', name='retail/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c4d7ae-8537-4f07-afcb-82cef21fa847",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "    .option('inferSchema',True)\\\n",
    "    .option('header',True)\\\n",
    "    .load('dbfs:/FileStore/tables/Online_Retail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa32a61e-ddb4-4a58-a3a8-721a12333ab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "076b6dd5-4d13-49e2-91a7-7b06915860ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This will let us know the default the Number of Partitions that we have (we can change this based on our needs to improve the performace)\n",
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53bcceab-7eeb-4408-a1b7-9fa04289fd44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This will let us know the max size of data that each partiton should have that is 128 MB (we can change this based on our needs to improve the performace)\n",
    "spark.conf.get(\"spark.sql.files.maxPartitionBytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9239046-6e28-49e4-a602-0e9a4ca4522b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1.  repartition()\n",
    "repartition(n) is a method used to change the number of partitions in a DataFrame by reshuffling the data, _**by using this function we ask spark to reshuffle this data into n evenly-sized chunks**_.\n",
    "\n",
    "### **Why Use repartition()?**\n",
    "- To increase or decrease the number of partitions.\n",
    "- To balance the data across partitions for better performance.\n",
    "- To prepare the data for joins, groupBy, or writing to disk efficiently.\n",
    "\n",
    "| Feature                  |   repartition()                          |\n",
    "| ------------------------ | ---------------------------------------- |\n",
    "| Shuffles data?           | ‚úÖ Yes (data is moved around the cluster) |\n",
    "| Can increase partitions? | ‚úÖ Yes                                    |\n",
    "| Can decrease partitions? | ‚úÖ Yes                                    |\n",
    "| Speed                    | üê¢ Slower (because of the shuffle)       |\n",
    "| Data Balance             | ‚úÖ Better (after shuffle)                 |\n",
    "\n",
    "\n",
    "\n",
    "## When Should You Use repartition()?\n",
    "‚úÖ Use it when:\n",
    "\n",
    "\n",
    "- You are increasing the number of partitions\n",
    "e.g., data is too big for current partitions, and you want more parallelism.\n",
    "- You‚Äôre preparing for a heavy operation like:\n",
    "join\n",
    "groupBy\n",
    "distinct\n",
    "These work better if data is evenly partitioned.\n",
    "- You want to avoid skew (data imbalance)\n",
    "\n",
    "‚ùå When Not to Use repartition()\n",
    "- If you're just reducing partitions, and performance matters ‚Üí use coalesce() instead (faster, no shuffle).\n",
    "\n",
    "\n",
    "\n",
    "### Imagine your data is a deck of cards:\n",
    "\n",
    "### Before:\n",
    "You have 2 piles (partitions) like this:\n",
    "\n",
    "\n",
    "- [‚ô†‚ô£‚ô•‚ô¶‚ô†‚ô£‚ô†] [‚ô•‚ô¶‚ô•‚ô¶‚ô£‚ô†‚ô¶]\n",
    "\n",
    "### repartition(4):\n",
    "Cards are reshuffled and split into 4 piles evenly:\n",
    "- [‚ô†‚ô¶] [‚ô£‚ô†] [‚ô•‚ô†] [‚ô¶‚ô£]\n",
    "### coalesce(2):\n",
    "You just merge existing piles (no shuffling):\n",
    "- [‚ô†‚ô£‚ô•‚ô¶‚ô†‚ô£‚ô†‚ô•] [‚ô¶‚ô•‚ô¶‚ô£‚ô†‚ô¶]\n",
    "\n",
    "### ‚ú® Bonus: Repartition by Column\n",
    "You can repartition based on a column, which helps group related data together:\n",
    "\n",
    "### df.repartition(10, \"category\")\n",
    "\n",
    "This tells Spark:\n",
    "\n",
    "- ‚ÄúPut all rows with the same category value into the same partition (as much as possible).‚Äù\n",
    "\n",
    "- ‚úÖ Helpful for joins and filtering!\n",
    "\n",
    "### Without Repartition ‚Äî Random Distribution:\n",
    "\n",
    "- Partition 1: [ A, C ]\n",
    "- Partition 2: [ B, A ]\n",
    "- Partition 3: [ C, B ]\n",
    "\n",
    "\n",
    "\n",
    "### With df.repartition(3, \"Category\"):\n",
    "\n",
    "- Partition 1: [ A, A, A ]\n",
    "- Partition 2: [ B, B, B ]\n",
    "- Partition 3: [ C, C ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fb66ed3-d146-41e4-a928-a188b465a211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 8\n"
     ]
    }
   ],
   "source": [
    "# Check current number of partitions\n",
    "print(\"Before:\", df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d12e0e-c9f8-48ff-b3ef-e045531795ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition to 10 partitions\n",
    "df2 = df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc3179ca-adfc-428e-8105-8a2ec442feeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"After:\", df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7b35ae-c9db-434b-8f35-6961f66a2e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: 8"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1266e3e0-3bf5-449b-a796-53be6e4b5560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Partitons by Category\n",
    "df2 = df.repartition(10,'Country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a341f5a-3841-4fb2-8bc8-229f5c3d4bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. coalesce()\n",
    "\n",
    "**coalesce(n)** is used to reduce the number of partitions without doing a full shuffle of the data , this function tells spark to _**combine some of the existing partitions to create fewer partitions ‚Äî but don't move data around too much**_.\"\n",
    "\n",
    "## Why Use coalesce()?\n",
    "- You want to reduce the number of partitions (usually before writing data to disk).\n",
    "- You want fewer output files.\n",
    "- You want to avoid a full shuffle (which repartition() does).\n",
    "\n",
    "## When is it Useful?\n",
    "‚úÖ Example: Writing Data\n",
    "- Let‚Äôs say Spark made 200 partitions, and you want 5 output files, not 200!\n",
    "### df.coalesce(5).write.parquet(\"output/\")\n",
    "‚úîÔ∏è This combines the 200 partitions into 5, without a heavy shuffle, and writes only 5 files.\n",
    "\n",
    "### Visual Example\n",
    "üîπ Before (6 Partitions):\n",
    "- [ A1 A2 ]   [ B1 ]   [ C1 ]   [ D1 D2 ]   [ E1 ]   [ F1 F2 ]\n",
    "### df.coalesce(3):\n",
    "- [ A1 A2 B1 ]   [ C1 D1 D2 ]   [ E1 F1 F2 ]\n",
    "#### ‚úîÔ∏è Spark merges existing partitions.\n",
    "#### ‚ùå Does not rebalance or reshuffle the data.\n",
    "\n",
    "### Key Difference vs repartition()\n",
    "\n",
    "| Feature                  | `coalesce()`                 | `repartition()`              |\n",
    "| ------------------------ | ---------------------------- | ---------------------------- |\n",
    "| Can increase partitions? | ‚ùå No                         | ‚úÖ Yes                        |\n",
    "| Can decrease partitions? | ‚úÖ Yes                        | ‚úÖ Yes                        |\n",
    "| Shuffles data?           | ‚ùå No shuffle                 | ‚úÖ Full shuffle               |\n",
    "| Speed                    | üöÄ Faster (no shuffle)       | üê¢ Slower (shuffle happens)  |\n",
    "| Balancing                | ‚ùå May be uneven              | ‚úÖ Balanced across partitions |\n",
    "| Use when                 | Reducing file count, writing | Balancing for joins/groupBy  |\n",
    "\n",
    "\n",
    "### üìå When Should You Use coalesce()?\n",
    "‚úÖ Best time to use:\n",
    "- Just before writing to disk, e.g., Parquet/CSV\n",
    "- When you want to reduce number of files\n",
    "\n",
    "### ‚ùå Avoid using coalesce():\n",
    "If you need evenly balanced partitions for processing ‚Üí use repartition() instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46bd9559-b18f-4967-ac04-4e330a93e15d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reduce the data to 2 partitions before writing\n",
    "df.coalesce(2).write.csv(\"FileStore/tables/retail/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47f95e88-60f5-48b0-99c0-96f5697c48db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üß† Pro Tip\n",
    "If you're unsure:\n",
    "\n",
    "‚úÖ Use coalesce() when reducing partitions before writing\n",
    "\n",
    "‚úÖ Use repartition() when processing and balancing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdae7363-4d3d-43c2-aab1-24a3ca421f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#3. combine repartition() and coalesce() \n",
    "we can combine repartition() and coalesce() in PySpark ‚Äî and in fact, this is a smart strategy in real-world data engineering.\n",
    "\n",
    "### Why Combine Them?\n",
    "Because they serve different purposes:\n",
    "\n",
    "- repartition() ‚Üí balances data (for processing)\n",
    "- coalesce() ‚Üí reduces partitions (for writing)\n",
    "\n",
    "\n",
    "### üîÅ What‚Äôs happening here?\n",
    "- repartition() helps Spark rebalance and optimize computation (like joins, groupBy, etc.)\n",
    "- coalesce() helps you avoid writing 100 tiny files by merging partitions before saving.\n",
    "\n",
    "### ‚ö†Ô∏è Important Notes\n",
    "- Always do repartition() before expensive operations.\n",
    "- Use coalesce() only before writing to disk.\n",
    "- Don‚Äôt use coalesce() if you're going to continue processing ‚Äî it's unbalanced and may slow things down.\n",
    "\n",
    "### üìä Summary\n",
    "| Use Case                    | Function Used                         |\n",
    "| --------------------------- | ------------------------------------- |\n",
    "| Balance data for joins      | `repartition()`                       |\n",
    "| Reduce file count           | `coalesce()`                          |\n",
    "| Combined use (best of both) | `repartition()` ‚Üí work ‚Üí `coalesce()` |\n",
    "\n",
    "\n",
    "The Typical Patterns is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae028e7b-63ba-4b3d-87b6-21bebaceac0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# 1. Repartition for balanced processing (e.g. for joins)\n",
    "df2 = df.repartition(100, \"Country\")\n",
    "\n",
    "# Do some heavy transformations...\n",
    "df2 = df2.groupBy(\"Country\").agg(sum(col('Quantity').cast(\"int\")).alias('TotalQuantity'))\n",
    "\n",
    "# 2. Coalesce to reduce output files (faster write)\n",
    "df2.coalesce(6).write.parquet(\"CountryQuantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80778049-3f84-48e8-8e6b-262edc022e45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Country</th><th>TotalQuantity</th></tr></thead><tbody><tr><td>Hong Kong</td><td>4769</td></tr><tr><td>Iceland</td><td>2458</td></tr><tr><td>Israel</td><td>4353</td></tr><tr><td>Channel Islands</td><td>9479</td></tr><tr><td>Sweden</td><td>35637</td></tr><tr><td>USA</td><td>1034</td></tr><tr><td>Cyprus</td><td>6317</td></tr><tr><td>Singapore</td><td>5234</td></tr><tr><td>Germany</td><td>117448</td></tr><tr><td>RSA</td><td>352</td></tr><tr><td>France</td><td>110480</td></tr><tr><td>Greece</td><td>1556</td></tr><tr><td>Saudi Arabia</td><td>75</td></tr><tr><td>Switzerland</td><td>30325</td></tr><tr><td>United Arab Emirates</td><td>982</td></tr><tr><td>European Community</td><td>497</td></tr><tr><td>Belgium</td><td>23152</td></tr><tr><td>Canada</td><td>2763</td></tr><tr><td>Finland</td><td>10666</td></tr><tr><td>Czech Republic</td><td>592</td></tr><tr><td>Brazil</td><td>356</td></tr><tr><td>Lebanon</td><td>386</td></tr><tr><td>Malta</td><td>944</td></tr><tr><td>Japan</td><td>25218</td></tr><tr><td>Poland</td><td>3653</td></tr><tr><td>Portugal</td><td>16180</td></tr><tr><td>Unspecified</td><td>3300</td></tr><tr><td>Australia</td><td>83653</td></tr><tr><td>Italy</td><td>7999</td></tr><tr><td>EIRE</td><td>142637</td></tr><tr><td>Austria</td><td>4827</td></tr><tr><td>Lithuania</td><td>652</td></tr><tr><td>Norway</td><td>19247</td></tr><tr><td>Spain</td><td>26824</td></tr><tr><td>Bahrain</td><td>260</td></tr><tr><td>Denmark</td><td>8188</td></tr><tr><td>United Kingdom</td><td>4263829</td></tr><tr><td>Netherlands</td><td>200128</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Hong Kong",
         4769
        ],
        [
         "Iceland",
         2458
        ],
        [
         "Israel",
         4353
        ],
        [
         "Channel Islands",
         9479
        ],
        [
         "Sweden",
         35637
        ],
        [
         "USA",
         1034
        ],
        [
         "Cyprus",
         6317
        ],
        [
         "Singapore",
         5234
        ],
        [
         "Germany",
         117448
        ],
        [
         "RSA",
         352
        ],
        [
         "France",
         110480
        ],
        [
         "Greece",
         1556
        ],
        [
         "Saudi Arabia",
         75
        ],
        [
         "Switzerland",
         30325
        ],
        [
         "United Arab Emirates",
         982
        ],
        [
         "European Community",
         497
        ],
        [
         "Belgium",
         23152
        ],
        [
         "Canada",
         2763
        ],
        [
         "Finland",
         10666
        ],
        [
         "Czech Republic",
         592
        ],
        [
         "Brazil",
         356
        ],
        [
         "Lebanon",
         386
        ],
        [
         "Malta",
         944
        ],
        [
         "Japan",
         25218
        ],
        [
         "Poland",
         3653
        ],
        [
         "Portugal",
         16180
        ],
        [
         "Unspecified",
         3300
        ],
        [
         "Australia",
         83653
        ],
        [
         "Italy",
         7999
        ],
        [
         "EIRE",
         142637
        ],
        [
         "Austria",
         4827
        ],
        [
         "Lithuania",
         652
        ],
        [
         "Norway",
         19247
        ],
        [
         "Spain",
         26824
        ],
        [
         "Bahrain",
         260
        ],
        [
         "Denmark",
         8188
        ],
        [
         "United Kingdom",
         4263829
        ],
        [
         "Netherlands",
         200128
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TotalQuantity",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Partitions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}